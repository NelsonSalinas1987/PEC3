---
title: "Práctica 2: Limpieza y validación de los datos"
authors: "Nelson Salinas Varas y Javier Botija Sanz"
date: "Mayo 2021"
output: 
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf_document:
    highlight: default
    number_section: yes
    toc: yes
    toc_depth: 3
  word_document:
    toc: yes
    toc_depth: '3'
lang: es
---


Para la correcta resolución de este documento, cargamos las librerías que vamos a utilizar a lo largo del mismo.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# cargamos las librería necesrias
library(ggplot2)
library(dplyr)
library(VIM)
library(nortest)
```



# Descripción del dataset. ¿Por qué es importante y qué pregunta/problema pretende responder?

El conjunto de datos que vamos a analizar es el conjunto de datos de enfermedades cardíacas del repositorio UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Heart+Disease).

Esta base de datos se compone de 4 datasets separados, de 4 ubicaciones diferentes (Cleveland, Hungría, Suiza y VA Long Beach). Se compone originalmente de 76 atributos, aunque están reducidos y procesados en subconjuntos de 14 características principales.

Esta base de datos contiene 76 atributos, pero todos los experimentos publicados se refieren al uso de un subconjunto de 14 de ellos. En particular, la base de datos de Cleveland es la única que han utilizado los investigadores hasta la fecha. 

Las 14 variables que se usan son las siguientes (# número original de los 76):

      -- 1. #3  (age)   age in years
      -- 2. #4  (sex)   sex (1 = male; 0 = female)
      -- 3. #9  (cp)    chest pain type
        -- Value 1: typical angina
        -- Value 2: atypical angina
        -- Value 3: non-anginal pain
        -- Value 4: asymptomatic        
      -- 4. #10 (trestbps)    resting blood pressure (in mm Hg on admission to the 
        hospital)
      -- 5. #12 (chol)    serum cholestoral in mg/dl    
      -- 6. #16 (fbs)   (fasting blood sugar > 120 mg/dl)  (1 = true; 0 = false)
      -- 7. #19 (restecg)   resting electrocardiographic results
        -- Value 0: normal
        -- Value 1: having ST-T wave abnormality (T wave inversions and/or ST 
                    elevation or depression of > 0.05 mV)
        -- Value 2: showing probable or definite left ventricular hypertrophy
                    by Estes' criteria   
      -- 8. #32 (thalach)   maximum heart rate achieved
      -- 9. #38 (exang)   exercise induced angina (1 = yes; 0 = no)     
      -- 10. #40 (oldpeak)    ST depression induced by exercise relative to rest
      -- 11. #41 (slope)    the slope of the peak exercise ST segment
        -- Value 1: upsloping
        -- Value 2: flat
        -- Value 3: downsloping     
      -- 12. #44 (ca)   number of major vessels (0-3) colored by flourosopy      
      -- 13. #51 (thal)   3 = normal; 6 = fixed defect; 7 = reversable defect   
      -- 14. #58 (num)    (the predicted attribute) diagnosis of heart disease (angiographic disease status)
        -- Value 0: < 50% diameter narrowing
        -- Value 1: > 50% diameter narrowing

Es una base de datos supervisada, y por tanto tiene una variable llamada 'num' con valores entre 0 y 4, que representa la presencia de una enfermedad cardíaca en el sujeto observado, siendo 0 la no enfermedad a 4 la más grave. Este estudio es importante, dado que ayudará a discernir, al menos, entre la presencia de una enfermedad (del 1 al 4) con respecto a la ausencia (0).

Hay que tener en cuenta que aunque todas las variables son numéricas, muchas de ellas son categóricas: sex, cp, fbs, restecg, exang, slope, ca, thal y la propia 'num'.

Hoy por hoy se potencia mucho este tipo de estudios, dado que conseguir predecir las enfermedades mediante recogida de variables es una de las mayores utilidades del análisis de datos. Puede ser una manera de diagnosticar precozmente enfermedades y actuar así preventinemente a ellas.

Lo que pretendemos realizar es un modelo que nos indique, mediante algunas variables,la posibilidad de contraer una enfermedad, y en qué grado.

# Integración y selección de los datos de interés a analizar.

Aunque la base de datos completa es de 4 subdatasets, la propia web nos advierte que los investigadores sólo han usado la base de datos de Cleveland. Vamos a observar por qué motivo es.

```{r,eval=TRUE,echo=TRUE}

# leemos los ficheros
clev <- read.csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data',
                      stringsAsFactors = FALSE,
                      sep = ',',
                      header = FALSE)
hung <- read.csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data',
                      stringsAsFactors = FALSE,
                      sep = ',',
                      header = FALSE)
swit <- read.csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.switzerland.data',
                      stringsAsFactors = FALSE,
                      sep = ',',
                      header = FALSE)
LBva <- read.csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.va.data',
                      stringsAsFactors = FALSE,
                      sep = ',',
                      header = FALSE)

# unimos todos los sub-dataset en uno único para poder trabajar con el
heart = rbind(clev,hung, swit, LBva)


# Dado que es un fichero sin cabeceras, cargamos los nombres de las columnas
names(heart) <- c("age","sex","cp","trestbps","chol","fbs","restecg","thalach","exang","oldpeak","slope","ca","thal","num")

# Vamos a mostrar la estructura del conjunto de datos
str(heart)

# valores básicos de cada variable
summary(heart)
```

La base de datos tiene valores nulos. Estos no son representados con un valor vacío, sino que se rellenan con un valor '?', por eso muchas de las columnas se han cargado como chr.

```{r}
# Sí hay valores ?, que no los trataremos dado que entendemos que son valores desconocidos
colSums(heart=="?")
```

Por este motivo, sabemos que no podemos unir el resto de subsets, dado que muchos de ellos carecen de un número de variables significativas.
Por tanto nos quedaremos con los 303 datos del subset de Cleveland, tal como se anticipaba.

```{r}
# nos quedamos sólo con los datos de Cleveland
heart = clev

# Dado que es un fichero sin cabeceras, cargamos los nombres de las columnas
names(heart) <- c("age","sex","cp","trestbps","chol","fbs","restecg","thalach","exang","oldpeak","slope","ca","thal","num")


# Vamos a mostrar la estructura del conjunto de datos
str(heart)
```


# Limpieza de los datos.

En este punto vamos a convertir todos los valores chr a numéricos:

```{r}

# pasamos a doble los campos con ?
#heart$trestbps <- as.double(heart$trestbps)
#heart$chol <- as.double(heart$chol)
#heart$fbs <- as.double(heart$fbs)
#heart$restecg <- as.double(heart$restecg)
#heart$thalach <- as.double(heart$thalach)
#heart$exang <- as.double(heart$exang)
#heart$oldpeak <- as.double(heart$oldpeak)
#heart$slope <- as.double(heart$slope)
heart$ca <- as.double(heart$ca)
heart$thal <- as.double(heart$thal)

# valores básicos de cada variable
summary(heart)

```

## ¿Los datos contienen ceros o elementos vacíos? ¿Cómo gestionarías cada uno de estos casos?

Aunque la función summary nos ha adelantado los valores NA, vamos a sacar sólo estos datos, asegurándonos que así es.

```{r}
 # Números de valores desconocidos por campo
sapply(heart, function(x) sum(is.na(x)))
```

Dado que tenemos muy pocos registros con datos nulos, podríamos optar por eliminar dichas filas, pero dado que nuestra base de datos no es muy numerosa en sujetos, optaremos por la imputación de valores. Podríamos optar por asignarles la media, pero en este caso utilizaremos el método de los KNN (k nearest neighborhoods) o k vecinos próximos, dado que se supone que tienen una relación entre ellos.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# aplicamos la imputación
heart$ca <- kNN(heart)$ca
heart$thal <- kNN(heart)$thal

# validamos que no quedan valores NA
sapply(heart, function(x) sum(is.na(x)))
```

## Identificación y tratamiento de valores extremos.

Para la identificación de valores extremos o outliers, los obtendremos gráficamente de las varibles que son continuas y no categóricas, y que los tienen (previo estudio de dichas variables) y también los obtendremos por la función de R que los lista.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# gráfico boxplot para obtener outliers 
boxplot(heart$trestbps)
# obtenemos los valores
boxplot.stats(heart$trestbps)$out

boxplot(heart$chol)
boxplot.stats(heart$chol)$out

boxplot(heart$thalach)
boxplot.stats(heart$thalach)$out

boxplot(heart$oldpeak)
boxplot.stats(heart$oldpeak)$out

```

En este caso, aunque los valores superan los XXX, ni los vamos a eliminar, ni los vamos a ponderar, dado que representan precisamente valores posibles y que pueden marcar la diferencia para indicar la enfermedad.

## Exportación de los datos procesados

```{r}
 # Exportación de los datos limpios en .csv
write.csv(heart, "heart_clean.csv")
```


# Análisis de los datos.

## Selección de los grupos de datos que se quieren analizar/comparar (planificación de los análisis a aplicar).

Para proceder al análisis, vamos a realizar diferentes grupos que permitirían comparar resultados.

```{r}
# Agrupación por sexo
heart.male <- heart[heart$sex == 1,] 
heart.female <- heart[heart$sex == 0,]

# Agrupación por fbs
heart.fbs <- heart[heart$fbs == 1,] 
heart.nofbs <- heart[heart$fbs == 0,]

# Agrupación por grupos de edad
heart.30 <- heart[heart$age < 40,] 
heart.40 <- heart[heart$age >= 40 & heart$age < 50,] 
heart.50 <- heart[heart$age >= 50 & heart$age < 60,]
heart.60 <- heart[heart$age >= 60 & heart$age < 70,]
heart.70 <- heart[heart$age >= 70,]


```

Podríamos realizar más segmentaciones de las mostradas, pero en este estudio, sólo vamos a utilizar la hecha por sexo. 

## Comprobación de la normalidad y homogeneidad de la varianza.

Para verificar que los valores se aproximan a una población distribuida, realizaremos la prueba de normalidad de Anderson-Darling, que consiste en obtener un p-valor superior a un nivel de significación (alfa) de 0,05. 

```{r}
alpha = 0.05
col.names = colnames(heart)

for (i in 1:ncol(heart)) {
  if (i == 1) cat("Variables que no siguen una distribución normal:\n") 
  if (is.integer(heart[,i]) | is.numeric(heart[,i])) {
    p_val = ad.test(heart[,i])$p.value 
    if (p_val < alpha) {
      cat(col.names[i])
      # Format output
      if (i < ncol(heart) - 1) cat(", ")
      if (i %% 3 == 0) cat("\n")
    }
  }
}
```

Seguidamente, pasamos a estudiar la homogeneidad de varianzas mediante la aplicación de un test de Fligner-Killeen. En este caso, estudiaremos esta homogeneidad en cuanto a los grupos por sexo y por fbs (glucemia en ayunas). 

En el siguiente test, considera que la hipótesis nula es que ambas varianzas son iguales.
```{r}
fligner.test(num ~ sex, data = heart)
```

Al obtener un p-valor inferior a 0,05, rechazamos la hipótesis nula y diremos que las varianzas no son igules.

Ahora lo haremos con el fbs:
```{r}
fligner.test(num ~ fbs, data = heart)
```

En este caso sí, al tener un p-valor superior a 0,05, aceptamos la hipótesis de que las varianzas de ambas muestras son homogéneas.

## Aplicación de pruebas estadísticas para comparar los grupos de datos. En función de los datos y el objetivo del estudio, aplicar pruebas de contraste de hipótesis, correlaciones, regresiones, etc. Aplicar al menos tres métodos de análisis diferentes.

### Contraste de hipótesis

En segundo lugar, vamos a realizar un contraste de hipótesis sobre uno de los segmentos antes realizados (por sexo), para determinar si la posibilidad de tener una enfermedad coronaria es diferente dependiendo del sexo del sujeto. Se suele decir que los hombres padecen más este tipo de enfermedades que las mujeres.

Lo primero que tendremos que validar es que nuestras muestras sean superiores a 30 (preferiblemente más que 50) para poder realizar el contraste.

```{r}
dim(heart.female)
dim(heart.male)
```

Dado que sí lo son, el contraste de hipótesis siguiente es válido.

La hipótesis que nos plantemos consistiría en:

• Hipótesis nula: para la variable que refleja el grado de enfermedad (num), el valor medio de los hombres es igual al de las mujeres

• Hipótesis alternativa: para dicha variable, el valor medio de los hombres es mayor que el de las mujeres.

O lo que es lo mismo, se plantea el siguiente contraste de hipótesis de dos muestras sobre la diferencia de medias, el cual es bilateral atendiendo a la formulación de la hipótesis alternativa:

  H0 : μ1 − μ2 = 0 
  H1 : μ1 − μ2 > 0

donde μ1 es la media de la enfermedad de las mujeres (población de la que se extrae la primera muestra) y μ2 es la media de la enfermedad de los hombres (población de la que extrae la segunda). Consideremos un nivel de confianza de 0.95 (α = 0, 05).

```{r}

t.test( x = heart.male$num, 
        y = heart.female$num, 
        alternative = "greater",
        conf.level  = 0.95)

```

El p-valor obtenido es menor que el valor de significación fijado, por tanto rechazamos la hipótesis nula. En este caso podemos concluir que, efectivamente, la posibilidad de tener mayor enfermedad en los hombres es mayor que en las mujeres.

Los valores son próximos, dado que son valores entre 0 y 4, pero la media es superior en hombres que en mujeres.

### Correlación

En primer lugar, procedemos a realizar un análisis de correlación entre las distintas variables para determinar cuáles de ellas ejercen una mayor influencia sobre el valor que determina la enfermedad. 

Se recorren cada culumna del dataset realizando el coeficiente de correlación de Spearman, puesto que ya hemos visto que nuestros datos que no siguen una distribución normal.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# creamos la matriz vacía
corr_matrix <- matrix(nc = 2, nr = 0) 
colnames(corr_matrix) <- c("estimate", "p-value")

# Calcular el coeficiente de correlación para cada variable cuantitativa 
# en nuestro caso todas, con respecto al valor que determina la enfermedad 
for (i in 1:(ncol(heart) - 1)) {
  if (is.integer(heart[,i]) | is.numeric(heart[,i])) {
    spearman_test = cor.test(heart[,i],
                            heart[,length(heart)],
                            method = "spearman")
    # creamos matriz de resultado
    pair = matrix(ncol = 2, nrow = 1)
    pair[1][1] = spearman_test$estimate
    pair[2][1] = spearman_test$p.value
    # la añadimos a la de resultados
    corr_matrix <- rbind(corr_matrix, pair)
    # nombramos la fila con el nombre de la columna
    rownames(corr_matrix)[nrow(corr_matrix)] <- colnames(heart)[i]
  }
}

# imprimimos la matriz
print(corr_matrix)
```
Podemos ver que no existe una variable que correlacione por sí misma con respecto al valor esperado.  Hay algunas que superan el 0.5, siendo este un valor bajo.

Si alguna variable, compo por ejemplo el fbs o glucemia en ayunas, es porque aunque es un valor numérico, realmente es un valor categórico (1  > 120 mg/dl, 0  <= 120 mg/dl). 

### Modelo de regresión lineal

Como planteábamos en el inicio de este documento, sería muy interesante poder predecir la posibilidad de tener enfermedades coronarias en base a algunas variables. Vamos a calcular varios modelos de regresión lineal utilizando diferentes variables, cuantitativas (continuas) y/o cualitativas (categóricas), obteniendo de cada uno de ellos el coeficiente de determinación (R2), para así saber cual se ajusta mejor.

Aunque partiremos de los datos de correlación de las variables obtenidos en el apartado anterior, añadiremos otros para ver si mejoran o no los modelos.

```{r}
# modelo con variables más correlacionadas
modelo1 <- lm(num ~ thal + ca + cp + oldpeak + thalach, data = heart)

# modelo con variables con outliers
modelo2 <- lm(num ~ trestbps + chol + thalach + oldpeak, data = heart)

# modelo con variables más correlacionadas sin outliers
modelo3 <- lm(num ~ thal + ca + cp, data = heart)

# modelo de todas las variables
modelo4 <- lm(num ~ age + sex + cp + trestbps + chol + fbs + restecg + thalach + exang + oldpeak + slope + ca + thal, data = heart)

tabla.r2 <- matrix(c(1, summary(modelo1)$r.squared, 
                      2, summary(modelo2)$r.squared,
                      3, summary(modelo3)$r.squared,
                      4, summary(modelo4)$r.squared), ncol = 2, byrow = TRUE)
colnames(tabla.r2) <- c("Modelo", "R2")

tabla.r2


```

Vemos que el último modelo es el que mejor coeficiente de determinación tiene, aunque está claro que el valor sigue siendo bajo.

# Representación de los resultados a partir de tablas y gráficas.


```{r}
Conf = matrix(c(1:2), nrow=1, byrow=TRUE)
layout(Conf)
boxplot(heart.female$num, main='female: num', col = 'lightblue' ) 
boxplot(heart.male$num, main='male: num', col = 'tomato' )
```

Completar.

# Resolución del problema. A partir de los resultados obtenidos, ¿cuáles son las conclusiones? ¿Los resultados permiten responder al problema?

En este ejercicio se ha realizado un análisis de una base de datos con valores numéricos que hemos tenido primero que completar, y también se ha realizado un análisis de datos outliers. Es importante tener claro cómo se cumplimentan los valores vacíos y cómo tratar los outliers si fuera el caso, dado que influyen mucho en los análisis posteriores.

Posteriormente hemos realizado un análisis de las variables más significativas. El análisis nos ha llevado a concluir que no existe una relación lineal ni univariante ni multivariante para concluir los datos. Es posible que para poder predecir correctamente las enfermedades tengamos que aplicar otros modelos.